{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "from libc.math cimport log, sqrt\n",
    "\n",
    "cdef inline float float_max(float a, float b): return a if a >= b else b\n",
    "cdef inline float float_min(float a, float b): return a if a <= b else b\n",
    "\n",
    "ctypedef np.float_t DTYPE_t\n",
    "\n",
    "@cython.cdivision(True)\n",
    "cpdef float cython_get_antropogenic_release(float xt, float c1, float c2, float r1, \n",
    "                             float r2, float w1):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xt : float\n",
    "         polution in lake at time t\n",
    "    c1 : float\n",
    "         center rbf 1\n",
    "    c2 : float\n",
    "         center rbf 2\n",
    "    r1 : float\n",
    "         ratius rbf 1\n",
    "    r2 : float\n",
    "         ratius rbf 2\n",
    "    w1 : float\n",
    "         weight of rbf 1\n",
    "         \n",
    "    note:: w2 = 1 - w1\n",
    "    \n",
    "    '''\n",
    "    cdef float rule, at, var1, var2, test\n",
    "    \n",
    "    var1 = (xt-c1)/r1\n",
    "    var1 = abs(var1)\n",
    "    var2 = (xt-c2)/r2\n",
    "    var2 = abs(var2)\n",
    "    \n",
    "    rule = w1*(var1**3)+(1-w1)*(var2**3)\n",
    "    at = float_min(float_max(rule, 0.01), 0.1)\n",
    "    return at\n",
    "    \n",
    "@cython.cdivision(True)\n",
    "@cython.boundscheck(False) \n",
    "def cython_lake_model(float b=0.42, float q=2.0, float mean=0.02, float stdev=0.001, \n",
    "               float alpha=0.4, float delta=0.98, float c1=0.25,\n",
    "               float c2=0.25, float r1=0.5, float r2=0.5, float w1=0.5, \n",
    "               int nsamples=100, int steps=100):    \n",
    "    '''runs the lake model for 1 stochastic realisation using specified \n",
    "    random seed.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    b : float\n",
    "        decay rate for P in lake (0.42 = irreversible)\n",
    "    q : float\n",
    "        recycling exponent\n",
    "    mean : float\n",
    "            mean of natural inflows\n",
    "    stdev : float\n",
    "            standard deviation of natural inflows\n",
    "    alpha : float\n",
    "            utility from pollution\n",
    "    delta : float\n",
    "            future utility discount rate\n",
    "    c1 : float\n",
    "    c2 : float\n",
    "    r1 : float\n",
    "    r2 : float\n",
    "    w1 : float\n",
    "    steps : int\n",
    "            the number of time steps (e.g., days)\n",
    "    seed : int, optional\n",
    "           seed for the random number generator\n",
    "    \n",
    "    '''\n",
    "    cdef float Pcrit, reliability, utility, inertia, transformed_mean, transformed_sigma\n",
    "    cdef int t\n",
    "    cdef np.ndarray[DTYPE_t, ndim=1] X = np.zeros([steps,], dtype=np.float)\n",
    "    cdef np.ndarray[DTYPE_t, ndim=1] decisions = np.zeros([steps,], dtype=np.float)\n",
    "    cdef np.ndarray[DTYPE_t, ndim=1] natural_inflows \n",
    "    \n",
    "#     np.random.seed(seed)\n",
    "    Pcrit = brentq(lambda x: x**q/(1+x**q) - b*x, 0.01, 1.5)\n",
    "    \n",
    "    transformed_mean = log(mean**2 / sqrt(stdev**2 + mean**2))\n",
    "    transformed_sigma = sqrt(log(1.0 + stdev**2 / mean**2))\n",
    "    natural_inflows = np.random.lognormal(transformed_mean, transformed_sigma, size=steps)\n",
    "\n",
    "    for t in range(1, steps):\n",
    "        decisions[t-1] = cython_get_antropogenic_release(X[t-1], c1, c2, r1, r2, \n",
    "                                                  w1)\n",
    "        X[t] = (1-b)*X[t-1] + X[t-1]**q/(1+X[t-1]**q) + decisions[t-1] + natural_inflows[t-1]\n",
    "\n",
    "    reliability = np.sum(X < Pcrit)/steps\n",
    "    utility = np.sum(alpha*decisions*np.power(delta,np.arange(steps)))\n",
    "    inertia = np.sum(np.abs(np.diff(decisions)) < 0.01)/(steps-1)\n",
    "    return X, utility, inertia, reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import (RealParameter, ScalarOutcome, Constant)\n",
    "from ema_workbench.em_framework.model import Replicator, BaseModel\n",
    "\n",
    "#instantiate the model\n",
    "class ReplicatorModel(Replicator, BaseModel):\n",
    "    pass\n",
    "lake_model = ReplicatorModel('lakeproblem', function=cython_lake_model)\n",
    "lake_model.replications = 150\n",
    "\n",
    "#specify uncertainties\n",
    "lake_model.uncertainties = [RealParameter('b', 0.1, 0.45),\n",
    "                            RealParameter('q', 2.0, 4.5),\n",
    "                            RealParameter('mean', 0.01, 0.05),\n",
    "                            RealParameter('stdev', 0.001, 0.005),\n",
    "                            RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# set levers\n",
    "lake_model.levers = [RealParameter(\"c1\", -2, 2),\n",
    "                     RealParameter(\"c2\", -2, 2),\n",
    "                     RealParameter(\"r1\", 0, 2), \n",
    "                     RealParameter(\"r2\", 0, 2), \n",
    "                     RealParameter(\"w1\", 0, 1)\n",
    "                     ]\n",
    "\n",
    "def process_p(values):\n",
    "    values = np.asarray(values)\n",
    "    values = np.mean(values, axis=0)\n",
    "    return np.max(values)\n",
    "\n",
    "#specify outcomes \n",
    "lake_model.outcomes = [ScalarOutcome('max_P', \n",
    "                                     kind=ScalarOutcome.MINIMIZE,\n",
    "                                     function=process_p),\n",
    "                       ScalarOutcome('utility', \n",
    "                                     kind=ScalarOutcome.MAXIMIZE,\n",
    "                                     function=np.mean),\n",
    "                       ScalarOutcome('inertia', \n",
    "                                     kind=ScalarOutcome.MAXIMIZE,\n",
    "                                     function=np.mean),\n",
    "                       ScalarOutcome('reliability', \n",
    "                                     kind=ScalarOutcome.MAXIMIZE,\n",
    "                                     function=np.mean)]\n",
    "\n",
    "# override some of the defaults of the model\n",
    "lake_model.constants = [Constant('alpha', 0.41),\n",
    "                        Constant('nsamples', 100),\n",
    "                        Constant('steps', 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many scenarios do we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import (MultiprocessingEvaluator, ema_logging, \n",
    "                           perform_experiments, save_results, load_results)\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "\n",
    "try:\n",
    "    load_results('./data/earning random policies.tar.gz')\n",
    "\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=1000, policies=10)\n",
    "    save_results(results, './data/learning random policies.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "experiments, outcomes = results\n",
    "\n",
    "\n",
    "\n",
    "for name, outcome in outcomes.items():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_ylabel(name)\n",
    "    \n",
    "    if name == 'max_P':\n",
    "        robustness_func = functools.partial(np.percentile, q=90)\n",
    "    else:\n",
    "        robustness_func = functools.partial(np.percentile, q=10)\n",
    "    \n",
    "    for i, policy in enumerate(set(experiments['policy'])):\n",
    "        logical = experiments['policy']==policy\n",
    "        data = outcome[logical]\n",
    "        robustness = []\n",
    "        for j in range(1, data.shape[0]):\n",
    "            robustness.append(robustness_func(data[0:j]))\n",
    "            \n",
    "        ax.plot(robustness, label=str(i))\n",
    "    ax.legend()\n",
    "plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.em_framework.optimization import to_dataframe\n",
    "from ema_workbench.em_framework.samplers import sample_uncertainties\n",
    "\n",
    "class Callback(object):\n",
    "    '''Callable object for tracking progress of optimization over generations\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.progress = []\n",
    "        self.archive_history = []\n",
    "\n",
    "    def __call__(self, optimizer):\n",
    "        self.progress.append(optimizer.algorithm.archive.improvements)\n",
    "        \n",
    "        dvnames = optimizer.problem.parameter_names\n",
    "        outcome_names = optimizer.problem.outcome_names\n",
    "        \n",
    "        self.archive_history.append(to_dataframe(optimizer, dvnames,\n",
    "                                                 outcome_names))\n",
    "callback = Callback()\n",
    "\n",
    "\n",
    "percentile10 = functools.partial(np.percentile, q=10)\n",
    "percentile90 = functools.partial(np.percentile, q=90)\n",
    "\n",
    "MAXIMIZE = ScalarOutcome.MAXIMIZE\n",
    "MINIMIZE = ScalarOutcome.MINIMIZE\n",
    "robustnes_functions = [ScalarOutcome('90th percentile max_p', kind=MINIMIZE, \n",
    "                             variable_name='max_P', function=percentile90),\n",
    "                       ScalarOutcome('10th percentile reliability', kind=MAXIMIZE, \n",
    "                             variable_name='reliability', function=percentile10),\n",
    "                       ScalarOutcome('10th percentile inertia', kind=MAXIMIZE, \n",
    "                             variable_name='inertia', function=percentile10),\n",
    "                       ScalarOutcome('10th percentile utility', kind=MAXIMIZE, \n",
    "                             variable_name='utility', function=percentile10)]\n",
    "\n",
    "\n",
    "n_scenarios = 200\n",
    "scenarios = sample_uncertainties(lake_model, n_scenarios)\n",
    "nfe = 100\n",
    "\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    robust_results = evaluator.robust_optimize(robustnes_functions, \n",
    "                            scenarios, nfe=nfe, population_size=25,\n",
    "                            epsilons=[0.05,]*len(robustnes_functions),\n",
    "                            callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
